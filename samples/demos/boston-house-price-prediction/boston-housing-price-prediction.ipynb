{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-19T06:37:26.0945Z","iopub.execute_input":"2023-09-19T06:37:26.094857Z","iopub.status.idle":"2023-09-19T06:37:26.56159Z","shell.execute_reply.started":"2023-09-19T06:37:26.094827Z","shell.execute_reply":"2023-09-19T06:37:26.560383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n#import statsmodels.api as sm\n#from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:37:26.564794Z","iopub.execute_input":"2023-09-19T06:37:26.565441Z","iopub.status.idle":"2023-09-19T06:37:30.510879Z","shell.execute_reply.started":"2023-09-19T06:37:26.565394Z","shell.execute_reply":"2023-09-19T06:37:30.509558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('HousingData.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:37:30.512332Z","iopub.execute_input":"2023-09-19T06:37:30.512667Z","iopub.status.idle":"2023-09-19T06:37:30.535002Z","shell.execute_reply.started":"2023-09-19T06:37:30.512638Z","shell.execute_reply":"2023-09-19T06:37:30.533904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first few rows of the dataset\nprint(\"====================\")\nprint(df.head())\n\n# Check the basic statistics of the numerical features\nprint(\"====================\")\nprint(df.describe())\n\n# Check for missing values\nprint(\"====================\")\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:37:30.537765Z","iopub.execute_input":"2023-09-19T06:37:30.53831Z","iopub.status.idle":"2023-09-19T06:37:30.61103Z","shell.execute_reply.started":"2023-09-19T06:37:30.538279Z","shell.execute_reply":"2023-09-19T06:37:30.610103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histograms of each feature\ndf.hist(bins=20, figsize=(14, 10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:37:30.612335Z","iopub.execute_input":"2023-09-19T06:37:30.612842Z","iopub.status.idle":"2023-09-19T06:37:33.527563Z","shell.execute_reply.started":"2023-09-19T06:37:30.612814Z","shell.execute_reply":"2023-09-19T06:37:33.526567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix heatmap\ncorrelation_matrix = df.corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True)\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:37:33.528841Z","iopub.execute_input":"2023-09-19T06:37:33.52921Z","iopub.status.idle":"2023-09-19T06:37:34.536162Z","shell.execute_reply.started":"2023-09-19T06:37:33.529178Z","shell.execute_reply":"2023-09-19T06:37:34.534931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plot of selected features against the target variable ('MEDV')\nplt.figure(figsize=(12, 6))\nsns.scatterplot(x='RM', y='MEDV', data=df)\nplt.xlabel('Average Number of Rooms (RM)')\nplt.ylabel('Median House Price (MEDV)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:37:34.537506Z","iopub.execute_input":"2023-09-19T06:37:34.537829Z","iopub.status.idle":"2023-09-19T06:37:35.002819Z","shell.execute_reply.started":"2023-09-19T06:37:34.537802Z","shell.execute_reply":"2023-09-19T06:37:35.001408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:37:35.004144Z","iopub.execute_input":"2023-09-19T06:37:35.004497Z","iopub.status.idle":"2023-09-19T06:37:35.013961Z","shell.execute_reply.started":"2023-09-19T06:37:35.004464Z","shell.execute_reply":"2023-09-19T06:37:35.012712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into features (X) and target (y)\nX = df.drop('MEDV', axis=1)  # Features\ny = df['MEDV']              # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:37:35.015342Z","iopub.execute_input":"2023-09-19T06:37:35.016759Z","iopub.status.idle":"2023-09-19T06:37:35.032319Z","shell.execute_reply.started":"2023-09-19T06:37:35.016707Z","shell.execute_reply":"2023-09-19T06:37:35.031361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define preprocessing steps for numerical and categorical features\nnumeric_features = X.select_dtypes(include=['number']).columns\nnumeric_transformer = StandardScaler()\n\ncategorical_features = []  # If you have categorical features, add them here\ncategorical_transformer = OneHotEncoder()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Define the model (in this case, we're using Linear Regression)\nmodel = LinearRegression()\n\n# Create a pipeline that includes preprocessing and the model\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', model)\n])\n\n# Fit the model to the training data\npipeline.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Calculate evaluation metrics\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)  # Taking the square root of MSE to get RMSE\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"MAE: {mae}\")\nprint(f\"MSE: {mse}\")\nprint(f\"RMSE: {rmse}\")\nprint(f\"R-squared (R2): {r2}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:37:35.035336Z","iopub.execute_input":"2023-09-19T06:37:35.036286Z","iopub.status.idle":"2023-09-19T06:37:35.088983Z","shell.execute_reply.started":"2023-09-19T06:37:35.036249Z","shell.execute_reply":"2023-09-19T06:37:35.088093Z"},"trusted":true},"execution_count":null,"outputs":[]}]}